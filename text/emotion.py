# from fastapi import FastAPI
# from pydantic import BaseModel
# import torch
# import torch.nn as nn
# from googletrans import Translator
# from torchtext.data import get_tokenizer
# import streamlit as st
# import asyncio
#
#
#
# news_app = FastAPI()
#
# classes = {
#     0: 'admiration',
#     1: 'amusement',
#     2: 'anger',
#     3: 'annoyance',
#     4: 'approval',
#     5: 'caring',
#     6: 'confusion',
#     7: 'curiosity',
#     8: 'desire',
#     9: 'disappointment',
#     10: 'disapproval',
#     11: 'disgust',
#     12: 'embarrassment',
#     13: 'excitement',
#     14: 'fear',
#     15: 'gratitude',
#     16: 'grief',
#     17: 'joy',
#     18: 'love',
#     19: 'nervousness',
#     20: 'optimism',
#     21: 'pride',
#     22: 'realization',
#     23: 'relief',
#     24: 'remorse',
#     25: 'sadness',
#     26: 'surprise',
#     27: 'neutral'
# }
#
#
# class CheckEmotion(nn.Module):
#   def __init__(self, vocab_size):
#     super().__init__()
#     self.emb = nn.Embedding(vocab_size, 64)
#     self.lstm = nn.LSTM(64, 128, batch_first=True)
#     self.lin = nn.Linear(128, 28)
#
#   def forward(self, x):
#     x = self.emb(x)
#     _, (h, c) = self.lstm(x)
#     h = h[-1]
#     x = self.lin(h)
#     return x
#
# vocab = torch.load('emotion_vocab.pth',  weights_only=False)
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = CheckEmotion(len(vocab))
# model.state_dict(torch.load('emotion_model.pth', map_location=device))
# model.to(device)
# model.eval()
#
# tokenizer = get_tokenizer('basic_english')
#
# def change_audio(text):
#     return [vocab[i] for i in tokenizer(text)]
#
# class TextSchema(BaseModel):
#     word: str
#
#
# translator = Translator()

# def emotion_text():
#     st.title('Emotion AI Model')
#     text = st.text_area("Input some text here", )
#     if st.button('Answer'):
#         async def translate_async(text):
#             return await translator.translate(text, dest='en')
#
#         if text:
#             translated = asyncio.run(translate_async(text))
#             translated_text = translated.text
#
#             num_text = torch.tensor(change_audio(translated_text), dtype=torch.int64).unsqueeze(0).to(device)
#             with torch.no_grad():
#                 pred = model(num_text)
#                 result = torch.argmax(pred, dim=1).item()
#                 st.success({'class': classes[result]})


from fastapi import FastAPI
from pydantic import BaseModel
import torch
import torch.nn as nn
from googletrans import Translator
from torchtext.data import get_tokenizer
import streamlit as st
import asyncio

# -------------------------------
# FastAPI instance
# -------------------------------
news_app = FastAPI()

# -------------------------------
# Class labels
# -------------------------------
classes = {
    0: 'admiration üòç',
    1: 'amusement üòÑ',
    2: 'anger üò°',
    3: 'annoyance üòí',
    4: 'approval üëç',
    5: 'caring ü§ó',
    6: 'confusion üòï',
    7: 'curiosity ü§î',
    8: 'desire üòè',
    9: 'disappointment üòû',
    10: 'disapproval üëé',
    11: 'disgust ü§Æ',
    12: 'embarrassment üò≥',
    13: 'excitement ü§©',
    14: 'fear üò®',
    15: 'gratitude üôè',
    16: 'grief üò¢',
    17: 'joy üòÅ',
    18: 'love ‚ù§Ô∏è',
    19: 'nervousness üò¨',
    20: 'optimism üòä',
    21: 'pride üòå',
    22: 'realization üí°',
    23: 'relief üòÖ',
    24: 'remorse üòî',
    25: 'sadness üò¢',
    26: 'surprise üò≤',
    27: 'neutral üòê'
}

# -------------------------------
# Model definition
# -------------------------------
class CheckEmotion(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, 64)
        self.lstm = nn.LSTM(64, 128, batch_first=True)
        self.lin = nn.Linear(128, 28)

    def forward(self, x):
        x = self.emb(x)
        _, (h, _) = self.lstm(x)
        h = h[-1]
        return self.lin(h)

# -------------------------------
# Load vocab + model
# -------------------------------
vocab = torch.load("emotion_vocab.pth", weights_only=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CheckEmotion(len(vocab))

model.load_state_dict(torch.load("emotion_model.pth", map_location=device))
model.to(device)
model.eval()

tokenizer = get_tokenizer("basic_english")

# -------------------------------
# Text to tensor
# -------------------------------
def change_audio(text: str):
    return [vocab[token] for token in tokenizer(text)]

# -------------------------------
# Pydantic schema
# -------------------------------
class TextSchema(BaseModel):
    word: str

translator = Translator()

# -------------------------------
# Streamlit UI
# -------------------------------
def emotion_text():
    st.title("Emotion AI Model")
    text = st.text_area("Input some text here:")

    if st.button("Answer"):
        async def translate_async(t):
            return await translator.translate(t, dest="en")

        if text:
            translated = asyncio.run(translate_async(text))
            translated_text = translated.text

            # Convert to tensor
            tokens = change_audio(translated_text)
            num_text = torch.tensor(tokens, dtype=torch.int64).unsqueeze(0).to(device)

            # Predict
            with torch.no_grad():
                pred = model(num_text)
                result = torch.argmax(pred, dim=1).item()

            st.success({"class": classes[result]})


def emotion_text():
    st.title("üòä Emotion Classification")

    st.markdown("""
    –≠—Ç–∞ –º–æ–¥–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç **—ç–º–æ—Ü–∏—é —Ç–µ–∫—Å—Ç–∞**, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å  
    **Embedding ‚Üí LSTM ‚Üí Linear**.

    –ú–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç **28 —ç–º–æ—Ü–∏–π**, –≤–∫–ª—é—á–∞—è:

    - ‚ù§Ô∏è love  
    - üòä joy  
    - üò° anger  
    - üò¢ sadness  
    - üò± surprise  
    - üòê neutral  
    - üòï confusion  
    - ü§© excitement  
    - üôè gratitude  
    - –∏ –¥—Ä—É–≥–∏–µ...

    –¢–µ–∫—Å—Ç –º–æ–∂–Ω–æ –≤–≤–æ–¥–∏—Ç—å **–Ω–∞ –ª—é–±–æ–º —è–∑—ã–∫–µ** ‚Äî –ø–µ—Ä–µ–¥ –∞–Ω–∞–ª–∏–∑–æ–º –æ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è.
    """)

    st.divider()

    st.subheader("üìù –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç")
    text = st.text_area(
        "–í–≤–µ–¥–∏—Ç–µ —Ñ—Ä–∞–∑—É –∏–ª–∏ –Ω–µ–±–æ–ª—å—à–æ–π –∞–±–∑–∞—Ü, –æ–ø–∏—Å—ã–≤–∞—é—â–∏–π —ç–º–æ—Ü–∏—é.",
        height=180
    )

    if st.button("üöÄ –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —ç–º–æ—Ü–∏—é", use_container_width=True, type="primary"):

        if not text.strip():
            st.warning("‚ö†Ô∏è –ü–æ–ª–µ –≤–≤–æ–¥–∞ –ø—É—Å—Ç–æ–µ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç.")
            return

        try:
            # ---- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ ----
            async def translate_async(tx):
                return await translator.translate(tx, dest="en")

            translated = asyncio.run(translate_async(text))
            translated_text = translated.text

            st.info(f"üåê –ü–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π: **{translated_text}**")

            # ---- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ----
            tokens = change_audio(translated_text)
            if not tokens:
                st.error("‚ö†Ô∏è –¢–µ–∫—Å—Ç –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.")
                return

            tensor = torch.tensor(tokens, dtype=torch.int64).unsqueeze(0).to(device)

            # ---- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ----
            with torch.no_grad():
                pred = model(tensor)
                cls = torch.argmax(pred, dim=1).item()

            st.success(f"üé≠ –û–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —ç–º–æ—Ü–∏—è: **{classes[cls]}**")

        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
