# from fastapi import FastAPI
# from pydantic import BaseModel
# import torch
# import torch.nn as nn
# from googletrans import Translator
# from torchtext.data import get_tokenizer
# import streamlit as st
# import asyncio
#
#
#
# news_app = FastAPI()
#
# classes = {
#     0: 'World',
#     1: 'Sport',
#     2: 'Business',
#     3: 'Sci/Tech'
# }
#
# class ChecNews(nn.Module):
#   def __init__(self, vocab_size):
#     super().__init__()
#     self.emb = nn.Embedding(vocab_size, 64)
#     self.lstm = nn.LSTM(64, 128, batch_first=True)
#     self.lin = nn.Linear(128, 4)
#
#   def forward(self, x):
#     x = self.emb(x)
#     _, (h, c) = self.lstm(x)
#     h = h[-1]
#     x = self.lin(h)
#     return x
#
# vocab = torch.load('news_vocab.pth',  weights_only=False)
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = ChecNews(len(vocab))
# model.state_dict(torch.load('news_model.pth', map_location=device))
# model.to(device)
# model.eval()
#
# tokenizer = get_tokenizer('basic_english')
#
# def change_audio(text):
#     return [vocab[i] for i in tokenizer(text)]
#
# class TextSchema(BaseModel):
#     word: str
#
#
# translator = Translator()
#
# def news_text():
#     st.title('News AI Model')
#     text = st.text_area("Input some text here", )
#     if st.button('Answer'):
#         async def translate_async(text):
#             return await translator.translate(text, dest='en')
#
#         if text:
#             translated = asyncio.run(translate_async(text))
#             translated_text = translated.text
#
#             num_text = torch.tensor(change_audio(translated_text), dtype=torch.int64).unsqueeze(0).to(device)
#             with torch.no_grad():
#                 pred = model(num_text)
#                 result = torch.argmax(pred, dim=1).item()
#                 st.success({'class': classes[result]})
#
import streamlit as st
import torch
import torch.nn as nn
import asyncio
from googletrans import Translator
from torchtext.data import get_tokenizer


# ------------------- –ú–æ–¥–µ–ª—å -------------------
class ChecNews(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, 64)
        self.lstm = nn.LSTM(64, 128, batch_first=True)
        self.lin = nn.Linear(128, 4)

    def forward(self, x):
        x = self.emb(x)
        _, (h, _) = self.lstm(x)
        h = h[-1]
        x = self.lin(h)
        return x


# ------------------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ -------------------
classes = {
    0: 'World üåç',
    1: 'Sport ‚öΩ',
    2: 'Business üíº',
    3: 'Sci/Tech üî¨'
}

vocab = torch.load('news_vocab.pth', weights_only=False)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = ChecNews(len(vocab))
model.load_state_dict(torch.load('news_model.pth', map_location=device))
model.to(device)
model.eval()

tokenizer = get_tokenizer('basic_english')
translator = Translator()


def encode_text(text):
    return [vocab[i] for i in tokenizer(text) if i in vocab]


# ------------------- –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å -------------------
def news_text():
    st.title("üì∞ News Classification AI")

    st.markdown("""
    –≠—Ç–∞ –º–æ–¥–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫ –∫–∞–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è **–Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Ç–µ–∫—Å—Ç**.  
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ **Embedding ‚Üí LSTM ‚Üí Linear**, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ AG-News.

    –ö–ª–∞—Å—Å—ã:
    - üåç *World*
    - ‚öΩ *Sport*
    - üíº *Business*
    - üî¨ *Sci/Tech*
    """)

    st.divider()

    st.subheader("üìù –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏")
    text = st.text_area(
        "–ü–æ–¥—Ö–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ª—é–±–æ–º —è–∑—ã–∫–µ ‚Äî —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–≤–µ–¥—ë—Ç.",
        height=200
    )

    if st.button("üöÄ –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤–æ—Å—Ç—å", use_container_width=True, type="primary"):

        if not text.strip():
            st.warning("‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç.")
            return

        try:
            # ---- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ ----
            async def translate_async(tx):
                return await translator.translate(tx, dest='en')

            translated = asyncio.run(translate_async(text))
            translated_text = translated.text

            st.info(f"üåê –ü–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π: **{translated_text}**")

            # ---- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ----
            encoded = encode_text(translated_text)
            if not encoded:
                st.error("‚ö†Ô∏è –ü–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç. –í–µ—Ä–æ—è—Ç–Ω–æ, —Å–ª–æ–≤–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ.")
                return

            tensor = torch.tensor(encoded, dtype=torch.int64).unsqueeze(0).to(device)

            # ---- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ----
            with torch.no_grad():
                pred = model(tensor)
                cls = torch.argmax(pred, dim=1).item()

            st.success(f"üß† –ö–ª–∞—Å—Å –Ω–æ–≤–æ—Å—Ç–∏: **{classes[cls]}**")

        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
