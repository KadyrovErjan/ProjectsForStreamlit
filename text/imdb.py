# from fastapi import FastAPI
# import torch
# import torch.nn as nn
# from googletrans import Translator
# from torchtext.data import get_tokenizer
# import streamlit as st
# import asyncio
# news_app = FastAPI()
#
# classes = {
#     0: 'Negative',
#     1: 'Positive',
# }
#
# class CheckIMDB(nn.Module):
#   def __init__(self, vocab_size):
#     super().__init__()
#     self.emb = nn.Embedding(vocab_size, 64)
#     self.lstm = nn.LSTM(64, 128, batch_first=True)
#     self.lin = nn.Linear(128, 2)
#
#   def forward(self, x):
#     x = self.emb(x)
#     _, (h, c) = self.lstm(x)
#     h = h[-1]
#     x = self.lin(h)
#     return x
#
# vocab = torch.load('imdb_vocab.pth',  weights_only=False)
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = CheckIMDB(len(vocab))
# model.state_dict(torch.load('imdb_model.pth', map_location=device))
# model.to(device)
# model.eval()
#
# tokenizer = get_tokenizer('basic_english')
#
# def change_audio(text):
#     return [vocab[i] for i in tokenizer(text)]
#
# # class TextSchema(BaseModel):
# #     word: str
#
#
# translator = Translator()
#
# # @news_app.post('/predict/')
# # async def check_text(text: TextSchema):
# def imdb_text():
#     st.title('IMDB AI Model')
#     text = st.text_area("Input some text here", )
#     if st.button('Answer'):
#         async def translate_async(text):
#             return await translator.translate(text, dest='en')
#
#         if text:
#             translated = asyncio.run(translate_async(text))
#             translated_text = translated.text
#
#             num_text = torch.tensor(change_audio(translated_text), dtype=torch.int64).unsqueeze(0).to(device)
#             with torch.no_grad():
#                 pred = model(num_text)
#                 result = torch.argmax(pred, dim=1).item()
#                 st.success({'class': classes[result]})
#
#
#
from fastapi import FastAPI
import torch
import torch.nn as nn
from googletrans import Translator
from torchtext.data import get_tokenizer
import streamlit as st
import asyncio

news_app = FastAPI()

# ------------------- –ö–ª–∞—Å—Å—ã -------------------
classes = {
    0: 'Negative üíî',
    1: 'Positive ‚ù§Ô∏è',
}


# ------------------- –ú–æ–¥–µ–ª—å -------------------
class CheckIMDB(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, 64)
        self.lstm = nn.LSTM(64, 128, batch_first=True)
        self.lin = nn.Linear(128, 2)

    def forward(self, x):
        x = self.emb(x)
        _, (h, _) = self.lstm(x)
        h = h[-1]
        x = self.lin(h)
        return x


# ------------------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ -------------------
vocab = torch.load("imdb_vocab.pth", weights_only=False)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = CheckIMDB(len(vocab))
model.load_state_dict(torch.load("imdb_model.pth", map_location=device))
model.to(device)
model.eval()

tokenizer = get_tokenizer("basic_english")
translator = Translator()


def encode_text(text):
    return [vocab[i] for i in tokenizer(text) if i in vocab]


# ------------------- –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å -------------------
def imdb_text():
    st.title("üé¨ IMDB Sentiment Analysis")

    st.markdown("""
    –≠—Ç–∞ –º–æ–¥–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç **—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ –æ—Ç–∑—ã–≤–∞ –æ —Ñ–∏–ª—å–º–µ**.  
    –û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ **Embedding ‚Üí LSTM ‚Üí Linear**.

    –ú–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    - ‚ù§Ô∏è **Positive**
    - üíî **Negative**
    """)

    st.divider()

    st.subheader("üìù –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞")
    text = st.text_area(
        "–ú–æ–∂–Ω–æ –≤–≤–æ–¥–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –ª—é–±–æ–º —è–∑—ã–∫–µ ‚Äî –º–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–≤–µ–¥—ë—Ç.",
        height=200
    )

    if st.button("üöÄ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–∑—ã–≤", use_container_width=True, type="primary"):

        if not text.strip():
            st.warning("‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç.")
            return

        try:
            # ---- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ ----
            async def translate_async(tx):
                return await translator.translate(tx, dest="en")

            translated = asyncio.run(translate_async(text))
            translated_text = translated.text

            st.info(f"üåê –ü–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π: **{translated_text}**")

            # ---- –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ ----
            encoded = encode_text(translated_text)
            if not encoded:
                st.error("‚ö†Ô∏è –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–ª–∞ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –°–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Å–ª–æ–≤–∞—Ä–µ.")
                return

            tensor = torch.tensor(encoded, dtype=torch.int64).unsqueeze(0).to(device)

            # ---- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ----
            with torch.no_grad():
                pred = model(tensor)
                cls = torch.argmax(pred, dim=1).item()

            st.success(f"üß† –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞: **{classes[cls]}**")

        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
