# from fastapi import FastAPI, HTTPException
# import io
# import torch
# from torchvision import transforms
# import torch.nn as nn
# from PIL import Image
# import streamlit as st
#
# transform_gray = transforms.Compose([
#     transforms.Resize((128, 128)),
#     transforms.Grayscale(),
#     transforms.ToTensor(),
# ])
#
# transform_rgb = transforms.Compose([
#     transforms.Resize((128, 128)),
#     transforms.ToTensor(),
# ])
#
#
# class CheckImageVGGGrey(nn.Module):
#     def __init__(self):
#         super().__init__()
#         self.first = nn.Sequential(
#             nn.Conv2d(1, 16, kernel_size=3, padding=1),
#             nn.BatchNorm2d(16),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#
#             nn.Conv2d(16, 32, kernel_size=3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#
#             nn.Conv2d(32, 64, kernel_size=3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#
#             nn.Conv2d(64, 128, kernel_size=3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#         )
#         self.second = nn.Sequential(
#             nn.Flatten(),
#             nn.Linear(128 * 8 * 8, 256),
#             nn.ReLU(),
#             nn.Linear(256, 6)
#         )
#
#     def forward(self, x):
#         x = self.first(x)
#         x = self.second(x)
#         return x
#
#
# class CheckImageVGGRGB(nn.Module):
#     def __init__(self):
#         super().__init__()
#         self.first = nn.Sequential(
#             nn.Conv2d(3, 64, kernel_size=3, padding=1),
#             nn.BatchNorm2d(64),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#
#             nn.Conv2d(64, 128, kernel_size=3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#
#             nn.Conv2d(128, 256, kernel_size=3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#
#             nn.Conv2d(256, 512, kernel_size=3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#         )
#         self.second = nn.Sequential(
#             nn.Flatten(),
#             nn.Linear(512 * 8 * 8, 1024),
#             nn.ReLU(),
#             nn.Linear(1024, 6)
#         )
#
#     def forward(self, x):
#         x = self.first(x)
#         x = self.second(x)
#         return x
#
# check_image_app = FastAPI()
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# model_gray= CheckImageVGGGrey()
# model_rgb = CheckImageVGGRGB()
# state_dict_gray = torch.load('model_gray_intel.pth', map_location=device)
# model_gray.load_state_dict(state_dict_gray, strict=False)
# model_gray.to(device)
# model_gray.eval()
# state_dict_rgb = torch.load('model_rgb_intel.pth', map_location=device)
# model_rgb.load_state_dict(state_dict_rgb, strict=False)
# model_rgb.to(device)
# model_rgb.eval()
#
# def intel_image():
#     name = st.radio("Choose input method:", ["GREY", "RGB"], horizontal=True)
#
#
#     class_names=['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']
#     if name == 'GREY':
#         st.title('Intel Image AI Classifier')
#         st.text('Upload image with a number, and model will recognize it')
#
#         file = st.file_uploader('Choose of drop an image', type=['svg', 'png', 'jpg', 'jpeg'])
#
#         if not file:
#             st.warning('No file is uploaded')
#         else:
#             st.image(file, caption='Uploaded image')
#             if st.button('Recognize the image'):
#                 try:
#                     image_data = file.read()
#                     if not image_data:
#                         raise HTTPException(status_code=400, detail='No image is given')
#                     img = Image.open(io.BytesIO(image_data))
#                     img_tensor = transform_gray(img).unsqueeze(0).to(device)
#
#                     with torch.no_grad():
#                         y_pred = model_gray(img_tensor)
#                         pred = y_pred.argmax(dim=1).item()
#
#                     st.success({'Prediction': class_names[pred]})
#
#                 except Exception as e:
#                     raise HTTPException(status_code=500, detail=str(e))
#
#     if name == 'RGB':
#         st.title('Intel Image AI Classifier')
#         st.text('Upload image with a number, and model will recognize it')
#
#         file = st.file_uploader('Choose of drop an image', type=['svg', 'png', 'jpg', 'jpeg'])
#
#         if not file:
#             st.warning('No file is uploaded')
#         else:
#             st.image(file, caption='Uploaded image')
#             if st.button('Recognize the image'):
#                 try:
#                     image_data = file.read()
#                     if not image_data:
#                         raise HTTPException(status_code=400, detail='No image is given')
#                     img = Image.open(io.BytesIO(image_data))
#                     img_tensor = transform_rgb(img).unsqueeze(0).to(device)
#
#
#                     with torch.no_grad():
#                         y_pred = model_rgb(img_tensor)
#                         pred = y_pred.argmax(dim=1).item()
#
#                     st.success({'Prediction': class_names[pred]})
#
#                 except Exception as e:
#                     raise HTTPException(status_code=500, detail=str(e))
#
#
#
import io
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
import streamlit as st
from fastapi import HTTPException

# ===========================
# üé® –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã Streamlit
# ===========================
st.set_page_config(
    page_title="Intel Image –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä",
    page_icon="üß†",
    layout="centered",
)

# ===========================
# ‚öôÔ∏è –ö–ª–∞—Å—Å—ã –º–æ–¥–µ–ª–µ–π
# ===========================
class CheckImageVGGGrey(nn.Module):
    def __init__(self):
        super().__init__()
        self.first = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.second = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 8 * 8, 256),
            nn.ReLU(),
            nn.Linear(256, 6)
        )

    def forward(self, x):
        x = self.first(x)
        x = self.second(x)
        return x


class CheckImageVGGRGB(nn.Module):
    def __init__(self):
        super().__init__()
        self.first = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.second = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512 * 8 * 8, 1024),
            nn.ReLU(),
            nn.Linear(1024, 6)
        )

    def forward(self, x):
        x = self.first(x)
        x = self.second(x)
        return x


# ===========================
# ‚öôÔ∏è –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
# ===========================
transform_gray = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.Grayscale(),
    transforms.ToTensor(),
])

transform_rgb = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])

# ===========================
# ‚öôÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
# ===========================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_gray = CheckImageVGGGrey().to(device)
model_rgb = CheckImageVGGRGB().to(device)

try:
    model_gray.load_state_dict(torch.load("model_gray_intel.pth", map_location=device), strict=False)
    model_rgb.load_state_dict(torch.load("model_rgb_intel.pth", map_location=device), strict=False)
except:
    st.warning("‚ö†Ô∏è –§–∞–π–ª—ã –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ .pth —Ñ–∞–π–ª—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.")

model_gray.eval()
model_rgb.eval()

# ===========================
# üß† –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
# ===========================
def intel_image():
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    st.markdown("<h1 style='text-align:center; color:#00ADB5;'>üèûÔ∏è Intel Image –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä</h1>", unsafe_allow_html=True)
    st.markdown("<p style='text-align:center; color:gray;'>AI-–º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞—ë—Ç —Ç–∏–ø –ª–∞–Ω–¥—à–∞—Ñ—Ç–∞ –∏–ª–∏ –≥–æ—Ä–æ–¥—Å–∫–æ–π —Å—Ä–µ–¥—ã.</p>", unsafe_allow_html=True)
    st.divider()

    # –û –º–æ–¥–µ–ª–∏
    with st.expander("‚ÑπÔ∏è –û –º–æ–¥–µ–ª–∏", expanded=False):
        st.write("""
        **Intel Image –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä** —Ä–∞—Å–ø–æ–∑–Ω–∞—ë—Ç 6 —Ç–∏–ø–æ–≤ —Å—Ü–µ–Ω:
        - üè¢ –ó–¥–∞–Ω–∏—è  
        - üå≤ –õ–µ—Å  
        - üßä –õ–µ–¥–Ω–∏–∫  
        - ‚õ∞Ô∏è –ì–æ—Ä—ã  
        - üåä –ú–æ—Ä–µ  
        - üõ£Ô∏è –£–ª–∏—Ü–∞  

        –ú–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ **GREY** –∏–ª–∏ **RGB**,  
        –∏ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∂–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π –∫–ª–∞—Å—Å.
        """)

    # –í—ã–±–æ—Ä —Ç–∏–ø–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    st.markdown("### üé® –í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    name = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –º–æ–¥–µ–ª–∏:", ["GREY", "RGB"], horizontal=True)
    class_names = ['–ó–¥–∞–Ω–∏—è', '–õ–µ—Å', '–õ–µ–¥–Ω–∏–∫', '–ì–æ—Ä—ã', '–ú–æ—Ä–µ', '–£–ª–∏—Ü–∞']

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    st.markdown("### üì§ –ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
    file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ –∏–ª–∏ –ø–µ—Ä–µ—Ç–∞—â–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", type=['png', 'jpg', 'jpeg'])

    if not file:
        st.info("üëÜ –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã.")
        st.stop()

    # –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    st.image(file, caption="–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", use_column_width=True)

    # –ö–Ω–æ–ø–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    if st.button("üîç –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å"):
        try:
            image_data = file.read()
            img = Image.open(io.BytesIO(image_data))

            if name == "GREY":
                img_tensor = transform_gray(img).unsqueeze(0).to(device)
                model = model_gray
            else:
                img_tensor = transform_rgb(img).unsqueeze(0).to(device)
                model = model_rgb

            with torch.no_grad():
                y_pred = model(img_tensor)
                pred = y_pred.argmax(dim=1).item()
                prediction = class_names[pred]

            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            st.success(f"‚úÖ **–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ:** {prediction}")
            st.progress((pred + 1) / len(class_names))

            # –≠–º–æ–¥–∑–∏ –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
            emoji_map = {
                "–ó–¥–∞–Ω–∏—è": "üè¢",
                "–õ–µ—Å": "üå≤",
                "–õ–µ–¥–Ω–∏–∫": "üßä",
                "–ì–æ—Ä—ã": "‚õ∞Ô∏è",
                "–ú–æ—Ä–µ": "üåä",
                "–£–ª–∏—Ü–∞": "üõ£Ô∏è",
            }
            st.markdown(f"<h3 style='text-align:center;'>{emoji_map[prediction]} {prediction}</h3>", unsafe_allow_html=True)

        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))


# ===========================
# üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
# ===========================
if __name__ == "__main__":
    intel_image()
