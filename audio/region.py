import streamlit as st
import torch
import torch.nn as nn
import torch.nn.functional as F
import io
import soundfile as sf
from torchaudio import transforms
from fastapi import FastAPI, HTTPException

# ------------------- ÐœÐ¾Ð´ÐµÐ»ÑŒ -------------------
class PlaceAudio(nn.Module):
    def __init__(self, num_classes=10):
        super(PlaceAudio, self).__init__()
        self.first = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.AdaptiveAvgPool2d((8, 8))
        )

        self.flatten = nn.Flatten()
        self.second = nn.Sequential(
            nn.Linear(64 * 8 * 8, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        if x.dim() == 3:
            x = x.unsqueeze(1)
        x = self.first(x)
        x = self.flatten(x)
        x = self.second(x)
        return x


# ------------------- ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ -------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

transform = transforms.MelSpectrogram(
    sample_rate=16000,
    n_fft=1024,
    win_length=1024,
    hop_length=256,
    n_mels=64,
    f_min=0,
    f_max=8000,
    power=2.0
)

labels = torch.load("region_labels.pth")
num_classes = len(labels)

model = PlaceAudio(num_classes=num_classes)
checkpoint = torch.load("region_model.pth", map_location=device)

missing, unexpected = model.load_state_dict(checkpoint, strict=False)
print("âš ï¸ ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð½Ñ‹Ðµ ÐºÐ»ÑŽÑ‡Ð¸:", missing)
print("âš ï¸ ÐÐµÐ¾Ð¶Ð¸Ð´Ð°Ð½Ð½Ñ‹Ðµ ÐºÐ»ÑŽÑ‡Ð¸:", unexpected)

model.to(device)
model.eval()

max_len = 500


# ------------------- ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð°ÑƒÐ´Ð¸Ð¾ -------------------
def change_audio_format(waveform, sample_rate):
    waveform = torch.tensor(waveform, dtype=torch.float32)
    if waveform.ndim > 1:
        waveform = waveform.mean(dim=1)

    if sample_rate != 16000:
        resample = transforms.Resample(orig_freq=sample_rate, new_freq=16000)
        waveform = resample(waveform)

    spec = transform(waveform).squeeze(0)

    if spec.shape[1] > max_len:
        spec = spec[:, :max_len]
    elif spec.shape[1] < max_len:
        spec = F.pad(spec, (0, max_len - spec.shape[1]))

    return spec


check_audio = FastAPI(title="Region Classifier")


# ------------------- Streamlit UI -------------------
def region_audio():
    st.title("ðŸŒ Region Audio Classification")
    st.markdown("""
    Ð­Ñ‚Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚, **Ð¸Ð· ÐºÐ°ÐºÐ¾Ð³Ð¾ Ñ€ÐµÐ³Ð¸Ð¾Ð½Ð° Ð¸Ð»Ð¸ ÑÑ‚Ñ€Ð°Ð½Ñ‹** Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð³Ð¾Ð»Ð¾Ñ Ð¸Ð»Ð¸ Ñ€ÐµÑ‡ÑŒ Ð½Ð° Ð°ÑƒÐ´Ð¸Ð¾Ð·Ð°Ð¿Ð¸ÑÐ¸.

    ðŸŽ§ ÐžÑÐ½Ð¾Ð²Ð°Ð½Ð° Ð½Ð° **ÑÐ²ÐµÑ€Ñ‚Ð¾Ñ‡Ð½Ð¾Ð¹ Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚Ð¸ (CNN)**, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ **Mel-ÑÐ¿ÐµÐºÑ‚Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ Ð·Ð²ÑƒÐºÐ°**  
    Ð¸ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð½Ñ‹Ðµ Ð°ÐºÑƒÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸, Ð¿Ñ€Ð¸ÑÑƒÑ‰Ð¸Ðµ Ñ€Ð°Ð·Ð½Ñ‹Ð¼ Ñ€ÐµÐ³Ð¸Ð¾Ð½Ð°Ð¼.
    """)

    st.divider()
    method = st.radio("ðŸŽ™ï¸ Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ ÑÐ¿Ð¾ÑÐ¾Ð± Ð²Ð²Ð¾Ð´Ð°:", ["ðŸ“¤ Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð°ÑƒÐ´Ð¸Ð¾", "ðŸŽ¤ Ð—Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ð³Ð¾Ð»Ð¾Ñ"], horizontal=True)
    st.divider()

    if method == "ðŸ“¤ Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð°ÑƒÐ´Ð¸Ð¾":
        st.subheader("ðŸ“¥ Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ WAV Ñ„Ð°Ð¹Ð»")
        audio_file = st.file_uploader("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ .wav Ñ„Ð°Ð¹Ð»", type="wav")

        if audio_file:
            st.audio(audio_file, format="audio/wav")
            if st.button("ðŸš€ ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ Ñ€ÐµÐ³Ð¸Ð¾Ð½", use_container_width=True, type="primary"):
                try:
                    data = audio_file.read()
                    wf, sr = sf.read(io.BytesIO(data), dtype="float32")
                    spec = change_audio_format(wf, sr).unsqueeze(0).to(device)

                    with torch.no_grad():
                        y_pred = model(spec)
                        pred_idx = torch.argmax(y_pred, dim=1).item()
                        pred_class = labels[pred_idx]

                    st.success(f"ðŸŒ ÐŸÑ€ÐµÐ´Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾, ÑÑ‚Ð¾ **{pred_class}** Ñ€ÐµÐ³Ð¸Ð¾Ð½")
                except Exception as e:
                    st.error(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
        else:
            st.info("ðŸ‘† Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ .wav Ñ„Ð°Ð¹Ð», Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ.")

    elif method == "ðŸŽ¤ Ð—Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ð³Ð¾Ð»Ð¾Ñ":
        st.subheader("ðŸŽ™ï¸ Ð—Ð°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ð¾Ð±Ñ€Ð°Ð·ÐµÑ† Ñ€ÐµÑ‡Ð¸")
        st.info(f"ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¼Ð¾Ð¶ÐµÑ‚ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð°Ñ‚ÑŒ Ñ€ÐµÐ³Ð¸Ð¾Ð½Ñ‹: {', '.join(labels[:10])} ...")

        audio_record = st.audio_input("ÐÐ°Ð¶Ð¼Ð¸Ñ‚Ðµ Ð´Ð»Ñ Ð·Ð°Ð¿Ð¸ÑÐ¸")

        if audio_record:
            st.audio(audio_record)
            if st.button("ðŸš€ ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ Ñ€ÐµÐ³Ð¸Ð¾Ð½", use_container_width=True, type="primary"):
                try:
                    data = audio_record.read()
                    wf, sr = sf.read(io.BytesIO(data), dtype="float32")
                    spec = change_audio_format(wf, sr).unsqueeze(0).to(device)

                    with torch.no_grad():
                        y_pred = model(spec)
                        pred_idx = torch.argmax(y_pred, dim=1).item()
                        pred_class = labels[pred_idx]

                    st.success(f"ðŸ§  ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´ÑƒÐ¼Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ ÑÑ‚Ð¾ **{pred_class}** Ñ€ÐµÐ³Ð¸Ð¾Ð½")
                except Exception as e:
                    st.error(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
        else:
            st.info("ðŸŽ¤ ÐÐ°Ð¶Ð¼Ð¸Ñ‚Ðµ ÐºÐ½Ð¾Ð¿ÐºÑƒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð·Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ ÑÐ²Ð¾Ð¹ Ð³Ð¾Ð»Ð¾Ñ.")
