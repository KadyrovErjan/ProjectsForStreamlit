# from fastapi import FastAPI
# import torch
# import torch.nn as nn
# from torchaudio import transforms
# import torch.nn.functional as F
# import io
# import soundfile as sf
# import streamlit as st
#
# class CheckAudio(nn.Module):
#     def __init__(self, num_classes=35):
#         super().__init__()
#         self.first = nn.Sequential(
#             nn.Conv2d(1, 16, kernel_size=3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#             nn.Conv2d(16, 32, kernel_size=3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#             nn.AdaptiveAvgPool2d((8, 8))
#         )
#
#         self.second = nn.Sequential(
#             nn.Flatten(),
#             nn.Linear(32 * 8 * 8, 128),
#             nn.ReLU(),
#             nn.Linear(128, 35),
#         )
#
#     def forward(self, x):
#         x = x.unsqueeze(1)
#         x = self.first(x)
#         x = self.second(x)
#         return x
#
#
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#
# labels = torch.load('speech_label.pth')
# model = CheckAudio()
# model.load_state_dict(torch.load('speech_model.pth', map_location=device))
# model.to(device)
# model.eval()
#
# transform = transforms.MelSpectrogram(
#     sample_rate=16000,
#     n_mels=64,
# )
#
# max_len = 100
#
#
# def change_audio_format(waveform, sample_rate):
#     if sample_rate != 16000:
#         new_sr = transforms.Resample(orig_freq=sample_rate, new_freq=16000)
#         waveform = new_sr(torch.tensor(waveform))
#
#     spec = transform(waveform).squeeze(0)
#
#     if spec.shape[1] > max_len:
#         spec = spec[:, :max_len]
#
#     elif spec.shape[1] < max_len:
#         count_diff = max_len - spec.shape[1]
#         spec = F.pad(spec, (0, count_diff))
#
#     return spec
#
#
# check_audio = FastAPI(title='Audio')
#
# def speech_audio():
#     name = st.radio("Choose input method:", ["Upload", "Record"], horizontal=True)
#     if name == 'Upload':
#         st.title("ðŸŽ§ Speech Commands")
#         st.text('Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ð°ÑƒÐ´Ð¸Ð¾ Ñ„Ð°Ð¹Ð»')
#
#         audio_file = st.file_uploader('Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ñƒ Ñ„Ð°Ð¹Ð»', type='wav')
#
#         if not audio_file:
#             st.warning('Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ .wav Ñ„Ð°Ð¹Ð»')
#         else:
#             st.audio(audio_file)
#         if st.button('Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ'):
#                 try:
#                     data =  audio_file.read()
#
#                     wf, sr = sf.read(io.BytesIO(data), dtype='float32')
#                     wf = torch.tensor(wf).T
#
#                     spec = change_audio_format(wf, sr).unsqueeze(0).to(device)
#
#                     with torch.no_grad():
#                         y_pred = model(spec)
#                         pred_idx = torch.argmax(y_pred, dim=1).item()
#                         pred_class = labels[pred_idx]
#                         st.success({'ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´ÑƒÐ¼Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ ÑÑ‚Ð¾ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð°': pred_class})
#
#                 except Exception as e:
#                     st.exception(f'{e}')
#
#
#     if name == 'Record':
#         st.title("ðŸŽ§ Speech Commands")
#         st.info(f'Ð¡ÐºÐ°Ð¶Ð¸ ÑÐ»Ð¾Ð²Ð¾ Ð¸Ð· ÑÑ‚Ð¾Ð³Ð¾ ÑÐ¿Ð¸ÑÐºÐ°: {labels}')
#
#         audio_record = st.audio_input('Ð¡ÐºÐ°Ð¶Ð¸Ñ‚Ðµ ÑÐ»Ð¾Ð²Ð¾')
#
#         st.audio(audio_record)
#         if st.button('Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ'):
#             try:
#                 data = audio_record.read()
#
#                 wf, sr = sf.read(io.BytesIO(data), dtype='float32')
#                 wf = torch.tensor(wf).T
#
#                 spec = change_audio_format(wf, sr).unsqueeze(0).to(device)
#
#                 with torch.no_grad():
#                     y_pred = model(spec)
#                     pred_idx = torch.argmax(y_pred, dim=1).item()
#                     pred_class = labels[pred_idx]
#                     st.success({'ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´ÑƒÐ¼Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ ÑÑ‚Ð¾ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð°': pred_class})
#
#             except Exception as e:
#                 st.exception(f'{e}')

import streamlit as st
import torch
import torch.nn.functional as F
import io
import soundfile as sf
from torchaudio import transforms
from fastapi import FastAPI
import torch.nn as nn


# ------------------- ÐœÐ¾Ð´ÐµÐ»ÑŒ -------------------
class CheckAudio(nn.Module):
    def __init__(self, num_classes=35):
        super().__init__()
        self.first = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.AdaptiveAvgPool2d((8, 8))
        )
        self.second = nn.Sequential(
            nn.Flatten(),
            nn.Linear(32 * 8 * 8, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = x.unsqueeze(1)
        x = self.first(x)
        x = self.second(x)
        return x


# ------------------- ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ -------------------
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

labels = torch.load('speech_label.pth')
model = CheckAudio()
model.load_state_dict(torch.load('speech_model.pth', map_location=device))
model.to(device)
model.eval()

transform = transforms.MelSpectrogram(sample_rate=16000, n_mels=64)
max_len = 100


def change_audio_format(waveform, sample_rate):
    if sample_rate != 16000:
        waveform = transforms.Resample(orig_freq=sample_rate, new_freq=16000)(torch.tensor(waveform))
    spec = transform(waveform).squeeze(0)
    if spec.shape[1] > max_len:
        spec = spec[:, :max_len]
    elif spec.shape[1] < max_len:
        spec = F.pad(spec, (0, max_len - spec.shape[1]))
    return spec


check_audio = FastAPI(title='Speech Commands Recognition')


# ------------------- Ð˜Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ -------------------
def speech_audio():
    st.title("ðŸ—£ï¸ Speech Commands Recognition")
    st.markdown("""
    Ð­Ñ‚Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚, **ÐºÐ°ÐºÐ¾Ðµ ÑÐ»Ð¾Ð²Ð¾ Ð²Ñ‹ Ð¿Ñ€Ð¾Ð¸Ð·Ð½ÐµÑÐ»Ð¸** Ð¸Ð· Ð½Ð°Ð±Ð¾Ñ€Ð° ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ñ… ÐºÐ¾Ð¼Ð°Ð½Ð´ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, "yes", "no", "stop", "go").

    ðŸŽ§ ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð° Ð½Ð° Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ðµ **Google Speech Commands**.  
    Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ **Mel-ÑÐ¿ÐµÐºÑ‚Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð° + ÑÐ²ÐµÑ€Ñ‚Ð¾Ñ‡Ð½Ð°Ñ Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÑŒ (CNN)**.
    """)

    st.divider()
    method = st.radio("ðŸŽ™ï¸ Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ ÑÐ¿Ð¾ÑÐ¾Ð± Ð²Ð²Ð¾Ð´Ð°:", ["ðŸ“¤ Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð°ÑƒÐ´Ð¸Ð¾", "ðŸŽ¤ Ð—Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ð³Ð¾Ð»Ð¾Ñ"], horizontal=True)
    st.divider()

    if method == "ðŸ“¤ Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð°ÑƒÐ´Ð¸Ð¾":
        st.subheader("ðŸ“¥ Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ WAV Ñ„Ð°Ð¹Ð»")
        audio_file = st.file_uploader("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ .wav Ñ„Ð°Ð¹Ð»", type="wav")

        if audio_file:
            st.audio(audio_file, format="audio/wav")
            if st.button("ðŸš€ Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ", use_container_width=True, type="primary"):
                try:
                    data = audio_file.read()
                    wf, sr = sf.read(io.BytesIO(data), dtype="float32")
                    wf = torch.tensor(wf).T

                    spec = change_audio_format(wf, sr).unsqueeze(0).to(device)
                    with torch.no_grad():
                        y_pred = model(spec)
                        pred_idx = torch.argmax(y_pred, dim=1).item()
                        pred_class = labels[pred_idx]

                    st.success(f"âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´ÑƒÐ¼Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ ÑÑ‚Ð¾ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð°: **{pred_class.upper()}**")
                except Exception as e:
                    st.error(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")

        else:
            st.info("ðŸ‘† Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ .wav Ñ„Ð°Ð¹Ð», Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ.")

    elif method == "ðŸŽ¤ Ð—Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ð³Ð¾Ð»Ð¾Ñ":
        st.subheader("ðŸŽ™ï¸ Ð—Ð°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾Ðµ ÑÐ»Ð¾Ð²Ð¾")
        st.info(f"ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð¾Ð´Ð½Ð¾ Ð¸Ð· ÑÐ»Ð¾Ð²: {', '.join(labels[:10])} ...")

        audio_record = st.audio_input("ÐÐ°Ð¶Ð¼Ð¸Ñ‚Ðµ Ð´Ð»Ñ Ð·Ð°Ð¿Ð¸ÑÐ¸")

        if audio_record:
            st.audio(audio_record)
            if st.button("ðŸš€ Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ", use_container_width=True, type="primary"):
                try:
                    data = audio_record.read()
                    wf, sr = sf.read(io.BytesIO(data), dtype="float32")
                    wf = torch.tensor(wf).T

                    spec = change_audio_format(wf, sr).unsqueeze(0).to(device)
                    with torch.no_grad():
                        y_pred = model(spec)
                        pred_idx = torch.argmax(y_pred, dim=1).item()
                        pred_class = labels[pred_idx]

                    st.success(f"ðŸ§  ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´ÑƒÐ¼Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ ÑÑ‚Ð¾ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð°: **{pred_class.upper()}**")
                except Exception as e:
                    st.error(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
        else:
            st.info("ðŸŽ¤ ÐÐ°Ð¶Ð¼Ð¸Ñ‚Ðµ ÐºÐ½Ð¾Ð¿ÐºÑƒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð·Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ ÑÐ²Ð¾Ð¹ Ð³Ð¾Ð»Ð¾Ñ.")
