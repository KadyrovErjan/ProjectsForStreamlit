# from fastapi import FastAPI, HTTPException, UploadFile, File
# import torch
# import torch.nn as nn
# from torchaudio import transforms
# import torch.nn.functional as F
# import io
# import soundfile as sf
# import streamlit as st
#
#
# class UrbanAudio(nn.Module):
#     def __init__(self, num_classes=10):
#         super(UrbanAudio, self).__init__()
#         self.first = nn.Sequential(
#             nn.Conv2d(1, 16, kernel_size=3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#
#             nn.Conv2d(16, 32, kernel_size=3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#
#             nn.Conv2d(32, 64, kernel_size=3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#
#             nn.AdaptiveAvgPool2d((8, 8))
#         )
#
#         self.flatten = nn.Flatten()
#
#         self.second = nn.Sequential(
#             nn.Linear(64 * 8 * 8, 128),
#             nn.ReLU(),
#             nn.Linear(128, 64),
#             nn.ReLU(),
#             nn.Linear(64, num_classes)
#         )
#
#     def forward(self, x):
#         x = x.unsqueeze(1)
#         x = self.first(x)
#         x = self.flatten(x)
#         x = self.second(x)
#         return x
#
#
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# sr = 22050
# transform = transforms.MelSpectrogram(
#     sample_rate=sr,
#     n_mels=64
# )
#
#
# labels = torch.load('urban_labels.pth')
#
# model = UrbanAudio()
# model.load_state_dict((torch.load('urban_model.pth', map_location=device)))
# model.to(device)
# model.eval()
#
# max_len = 500
#
# def change_audio(waveform, sample_rate):
#     # waveform: torch.Tensor [samples]
#     if sample_rate != sr:
#         resample = transforms.Resample(orig_freq=sample_rate, new_freq=sr)
#         # waveform Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð¸Ð¼ÐµÑ‚ÑŒ Ñ„Ð¾Ñ€Ð¼Ñƒ [1, N] Ð´Ð»Ñ resample
#         if waveform.ndim == 1:
#             waveform = waveform.unsqueeze(0)
#         waveform = resample(waveform)
#         waveform = waveform.squeeze(0)
#
#     # ÑÑ‚Ñ€Ð¾Ð¸Ð¼ Ð¼ÐµÐ»-ÑÐ¿ÐµÐºÑ‚Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ
#     spec = transform(waveform).squeeze(0)
#
#     # Ð²Ñ‹Ñ€Ð°Ð²Ð½Ð¸Ð²Ð°ÐµÐ¼ Ð´Ð»Ð¸Ð½Ñƒ
#     if spec.shape[1] > max_len:
#         spec = spec[:, :max_len]
#     elif spec.shape[1] < max_len:
#         spec = F.pad(spec, (0, max_len - spec.shape[1]))
#
#     return spec
#
#
#
# torch_audio = FastAPI(title='Urban sounds')
#
#
#
# def urban_audio():
#     st.title('Model Urban')
#     st.text('Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ð°ÑƒÐ´Ð¸Ð¾ Ñ„Ð°Ð¹Ð»')
#
#     audio_file = st.file_uploader('Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ñƒ Ñ„Ð°Ð¹Ð»', type='wav')
#
#     if not audio_file:
#         st.warning('Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ .wav Ñ„Ð°Ð¹Ð»')
#     else:
#         st.audio(audio_file)
#     if st.button('Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ'):
#         try:
#             data = audio_file.read()
#             if not data:
#                 raise HTTPException(status_code=400, detail='Empty file')
#
#             waveform, sample_rate = sf.read(io.BytesIO(data), dtype='float32')
#             waveform = torch.tensor(waveform, dtype=torch.float32)
#             if waveform.ndim > 1:
#                 waveform = waveform.mean(dim=1)  # Ð´ÐµÐ»Ð°ÐµÐ¼ Ð¼Ð¾Ð½Ð¾
#
#             spec = change_audio(waveform, sample_rate).unsqueeze(0).to(device)
#
#             with torch.no_grad():
#                 y_pred = model(spec)
#                 pred_idx = torch.argmax(y_pred, dim=1).item()
#                 predicted_class = labels[pred_idx]
#
#             st.success({"Index": pred_idx, "Sound": predicted_class})
#
#         except Exception as e:
#             st.exception(e)


from fastapi import FastAPI, HTTPException
import torch
import torch.nn as nn
from torchaudio import transforms
import torch.nn.functional as F
import io
import soundfile as sf
import streamlit as st


# ==================== ðŸŽµ ÐœÐ¾Ð´ÐµÐ»ÑŒ ====================
class UrbanAudio(nn.Module):
    def __init__(self, num_classes=10):
        super(UrbanAudio, self).__init__()
        self.first = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.AdaptiveAvgPool2d((8, 8))
        )

        self.flatten = nn.Flatten()
        self.second = nn.Sequential(
            nn.Linear(64 * 8 * 8, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        x = x.unsqueeze(1)
        x = self.first(x)
        x = self.flatten(x)
        x = self.second(x)
        return x


# ==================== âš™ï¸ ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ ====================
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
SAMPLE_RATE = 22050
MAX_LEN = 500

transform = transforms.MelSpectrogram(
    sample_rate=SAMPLE_RATE,
    n_mels=64
)

labels = torch.load('urban_labels.pth')

model = UrbanAudio(num_classes=len(labels))
model.load_state_dict(torch.load('urban_model.pth', map_location=device))
model.to(device)
model.eval()

torch_audio = FastAPI(title="Urban Sounds Classifier")


# ==================== ðŸŽ§ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð°ÑƒÐ´Ð¸Ð¾ ====================
def change_audio(waveform, sample_rate):
    """ÐŸÑ€Ð¸Ð²Ð¾Ð´Ð¸Ð¼ Ð°ÑƒÐ´Ð¸Ð¾ Ðº Ð½ÑƒÐ¶Ð½Ð¾Ð¼Ñƒ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ñƒ Ð¸ ÑÑ‚Ñ€Ð¾Ð¸Ð¼ Ð¼ÐµÐ»-ÑÐ¿ÐµÐºÑ‚Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ"""
    waveform = torch.tensor(waveform, dtype=torch.float32)

    # Ð’ Ð¼Ð¾Ð½Ð¾
    if waveform.ndim > 1:
        waveform = waveform.mean(dim=1)

    # Ð ÐµÑÐµÐ¼Ð¿Ð»Ð¸Ð½Ð³
    if sample_rate != SAMPLE_RATE:
        resample = transforms.Resample(orig_freq=sample_rate, new_freq=SAMPLE_RATE)
        waveform = resample(waveform.unsqueeze(0)).squeeze(0)

    # Ð¡Ð¿ÐµÐºÑ‚Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð°
    spec = transform(waveform).squeeze(0)

    # ÐŸÐ¾Ð´Ð³Ð¾Ð½ÑÐµÐ¼ Ð´Ð»Ð¸Ð½Ñƒ
    if spec.shape[1] > MAX_LEN:
        spec = spec[:, :MAX_LEN]
    elif spec.shape[1] < MAX_LEN:
        spec = F.pad(spec, (0, MAX_LEN - spec.shape[1]))

    return spec


# ==================== ðŸš€ Ð˜Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ Streamlit ====================
def urban_audio():
    st.set_page_config(page_title="ðŸ™ï¸ Urban Sound Classifier", layout="centered")

    st.title("ðŸ™ï¸ UrbanSound8K Classifier")
    st.markdown(
        """
        ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ **Ñ‚Ð¸Ð¿ Ð·Ð²ÑƒÐºÐ° Ð¸Ð· Ð³Ð¾Ñ€Ð¾Ð´Ð°** Ð¿Ð¾ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾Ð¼Ñƒ Ð°ÑƒÐ´Ð¸Ð¾ ðŸŽ§  
        ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹: ðŸš— ÐœÐ°ÑˆÐ¸Ð½Ð°, ðŸ• Ð›Ð°Ð¹ ÑÐ¾Ð±Ð°ÐºÐ¸, ðŸš¨ Ð¡Ð¸Ñ€ÐµÐ½Ð°, ðŸ”¨ Ð¡Ñ‚Ñ€Ð¾Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð¾ Ð¸ Ñ‚.Ð´.
        """
    )

    st.divider()
    st.subheader("ðŸ“‚ Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ WAV-Ñ„Ð°Ð¹Ð»")

    audio_file = st.file_uploader("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð°ÑƒÐ´Ð¸Ð¾Ñ„Ð°Ð¹Ð»", type=["wav"])

    if not audio_file:
        st.info("ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð²Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ .wav Ñ„Ð°Ð¹Ð» Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°.")
        return

    st.audio(audio_file, format="audio/wav")

    if st.button("ðŸ” Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ Ð·Ð²ÑƒÐº", use_container_width=True):
        try:
            data = audio_file.read()
            if not data:
                raise HTTPException(status_code=400, detail="ÐŸÑƒÑÑ‚Ð¾Ð¹ Ñ„Ð°Ð¹Ð»")

            waveform, sample_rate = sf.read(io.BytesIO(data), dtype='float32')

            spec = change_audio(waveform, sample_rate).unsqueeze(0).to(device)

            with torch.no_grad():
                y_pred = model(spec)
                pred_idx = torch.argmax(y_pred, dim=1).item()
                predicted_class = labels[pred_idx]

            st.success(f"âœ… ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»Ñ‘Ð½Ð½Ñ‹Ð¹ Ð·Ð²ÑƒÐº: **{predicted_class}**")
            st.caption(f"ðŸŽšï¸ Ð˜Ð½Ð´ÐµÐºÑ ÐºÐ»Ð°ÑÑÐ°: {pred_idx}")

        except Exception as e:
            st.error("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Ñ„Ð°Ð¹Ð»Ð°:")
            st.exception(e)


# ==================== ðŸ§© Ð—Ð°Ð¿ÑƒÑÐº ====================
if __name__ == "__main__":
    urban_audio()
