# from fastapi import FastAPI, HTTPException, UploadFile, File
# import torch
# import torch.nn as nn
# from torchaudio import transforms
# import torch.nn.functional as F
# import io
# import soundfile as sf
# import streamlit as st
#
# class CheckAudio(nn.Module):
#   def __init__(self):
#     super().__init__()
#     self.first = nn.Sequential(
#         nn.Conv2d(1, 16, kernel_size=3, padding=1),
#         nn.ReLU(),
#         nn.MaxPool2d(2),
#         nn.Conv2d(16, 32, kernel_size=3, padding=1),
#         nn.ReLU(),
#         nn.MaxPool2d(2),
#         nn.AdaptiveAvgPool2d((8, 8)),
#     )
#
#     self.second = nn.Sequential(
#         nn.Flatten(),
#         nn.Linear(32 * 8 * 8, 128),
#         nn.ReLU(),
#         nn.Linear(128, 10)
#     )
#
#
#   def forward(self, x):
#     x = x.unsqueeze(1)
#     x = self.first(x)
#     x = self.second(x)
#     return x
#
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#
# transform = transforms.MelSpectrogram(
#     sample_rate=22050,
#     n_mels=64
# )
# max_len = 500
#
# genres = torch.load('gtzan_labels.pth')
# index_to_label = {ind: lab for ind, lab in enumerate(genres)}
#
# model = CheckAudio()
# model.load_state_dict(torch.load('gtzan_model.pth', map_location=device))
# model.to(device)
# model.eval()
#
# def change_audio(waveform, sr):
#     if sr != 22050:
#         resample = transforms.Resample(orig_freq=sr, new_freq=22050)
#         waveform = resample(torch.tensor(waveform).unsqueeze(0))
#
#     spec = transform(waveform).squeeze(0)
#
#     if spec.shape[1] > max_len:
#         spec = spec[:, :max_len]
#
#     if spec.shape[1] < max_len:
#         count_len = max_len - spec.shape[1]
#         spec = F.pad(spec, (0, count_len))
#
#     return spec
# audio_app = FastAPI()
#
#
# def gtzan_audio():
#     st.title('Model GTZAN')
#     st.text('Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ð°ÑƒÐ´Ð¸Ð¾ Ñ„Ð°Ð¹Ð»')
#
#     audio_file = st.file_uploader('Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ñƒ Ñ„Ð°Ð¹Ð»', type='wav')
#
#     if not audio_file:
#         st.warning('Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ .wav Ñ„Ð°Ð¹Ð»')
#     else:
#         st.audio(audio_file)
#     if st.button('Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ'):
#             try:
#                 data = audio_file.read()
#                 if not data:
#                     raise HTTPException(status_code=404, detail='ÐŸÑƒÑÑ‚Ð¾Ð¹ Ñ„Ð°Ð¹Ð»')
#                 wf, sr = sf.read(io.BytesIO(data), dtype='float32')
#                 wf = torch.tensor(wf).T
#
#                 spec = change_audio(wf, sr).unsqueeze(0).to(device)
#
#                 with torch.no_grad():
#                     y_pred = model(spec)
#                     pred_ind = torch.argmax(y_pred, dim=1).item()
#                     pred_class = index_to_label[pred_ind]
#
#                 st.success({f'Ð˜Ð½Ð´ÐµÐºÑ: {pred_ind}  Ð–Ð°Ð½Ñ€: {pred_class}'})
#             except Exception as e:
#                 st.exception(f'{e}')

from fastapi import FastAPI, HTTPException
import torch
import torch.nn as nn
from torchaudio import transforms
import torch.nn.functional as F
import io
import soundfile as sf
import streamlit as st

# ==================== ðŸŽ¶ ÐœÐ¾Ð´ÐµÐ»ÑŒ ====================
class CheckAudio(nn.Module):
    def __init__(self):
        super().__init__()
        self.first = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.AdaptiveAvgPool2d((8, 8)),
        )

        self.second = nn.Sequential(
            nn.Flatten(),
            nn.Linear(32 * 8 * 8, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )

    def forward(self, x):
        x = x.unsqueeze(1)
        x = self.first(x)
        x = self.second(x)
        return x


# ==================== âš™ï¸ ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ ====================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

transform = transforms.MelSpectrogram(
    sample_rate=22050,
    n_mels=64
)
max_len = 500

genres = torch.load('gtzan_labels.pth')
index_to_label = {ind: lab for ind, lab in enumerate(genres)}

model = CheckAudio()
model.load_state_dict(torch.load('gtzan_model.pth', map_location=device))
model.to(device)
model.eval()


# ==================== ðŸŽ§ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð°ÑƒÐ´Ð¸Ð¾ ====================
def change_audio(waveform, sr):
    if sr != 22050:
        resample = transforms.Resample(orig_freq=sr, new_freq=22050)
        waveform = resample(torch.tensor(waveform).unsqueeze(0))

    spec = transform(waveform).squeeze(0)

    if spec.shape[1] > max_len:
        spec = spec[:, :max_len]
    elif spec.shape[1] < max_len:
        spec = F.pad(spec, (0, max_len - spec.shape[1]))

    return spec


# ==================== ðŸš€ FastAPI + Streamlit ====================
audio_app = FastAPI()


def gtzan_audio():
    st.set_page_config(page_title="ðŸŽµ GTZAN Genre Classifier", layout="centered")

    st.title("ðŸŽ§ GTZAN Music Genre Classifier")
    st.markdown(
        """
        ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ **Ð¶Ð°Ð½Ñ€ Ð¼ÑƒÐ·Ñ‹ÐºÐ¸** Ð¿Ð¾ Ð²Ð°ÑˆÐµÐ¼Ñƒ Ð°ÑƒÐ´Ð¸Ð¾Ñ„Ð°Ð¹Ð»Ñƒ.  
        ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÐ¼Ñ‹Ðµ Ð¶Ð°Ð½Ñ€Ñ‹:  
        ðŸŽ¸ Rock â€¢ ðŸŽ¹ Classical â€¢ ðŸŽ¤ Pop â€¢ ðŸ¥ HipHop â€¢ ðŸŽº Jazz â€¢ Ð¸ Ð´Ñ€ÑƒÐ³Ð¸Ðµ
        """
    )

    with st.container():
        st.subheader("ðŸ“‚ Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ WAV-Ñ„Ð°Ð¹Ð»")
        audio_file = st.file_uploader("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ñ„Ð°Ð¹Ð» (.wav)", type=["wav"])

        if not audio_file:
            st.info("ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ð°ÑƒÐ´Ð¸Ð¾ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°.")
        else:
            st.audio(audio_file, format='audio/wav')

        if st.button("ðŸ” Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ Ð¶Ð°Ð½Ñ€", use_container_width=True):
            try:
                data = audio_file.read()
                if not data:
                    raise HTTPException(status_code=400, detail="ÐŸÑƒÑÑ‚Ð¾Ð¹ Ñ„Ð°Ð¹Ð»")

                wf, sr = sf.read(io.BytesIO(data), dtype='float32')
                wf = torch.tensor(wf).T

                spec = change_audio(wf, sr).unsqueeze(0).to(device)

                with torch.no_grad():
                    y_pred = model(spec)
                    pred_ind = torch.argmax(y_pred, dim=1).item()
                    pred_class = index_to_label[pred_ind]

                st.success(f"ðŸŽ¶ **ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ð¹ Ð¶Ð°Ð½Ñ€:** `{pred_class}`")
                st.caption(f"ðŸ“Š Ð˜Ð½Ð´ÐµÐºÑ ÐºÐ»Ð°ÑÑÐ°: {pred_ind}")

            except Exception as e:
                st.error("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Ñ„Ð°Ð¹Ð»Ð°")
                st.exception(e)


# ==================== ðŸ§© Ð—Ð°Ð¿ÑƒÑÐº ====================
if __name__ == "__main__":
    gtzan_audio()
